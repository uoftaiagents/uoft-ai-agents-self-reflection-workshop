{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔬 Research Extensions: Next-Level AI\n",
    "**UofT AI Agents Club - Advanced Track**\n",
    "\n",
    "Ready to explore cutting-edge AI research? Let's implement some **real research ideas**!\n",
    "\n",
    "## What's Here\n",
    "1. **Meta-Reflection** - Agents that critique their own critique process\n",
    "2. **Smart Evaluation** - Advanced ways to measure response quality\n",
    "3. **Research Ideas** - Concepts from recent AI papers\n",
    "4. **Your Research** - Design your own experiments\n",
    "\n",
    "**Prerequisites**: Complete both previous notebooks first\n",
    "\n",
    "**Note**: This is experimental - some things might not work perfectly!\n",
    "\n",
    "Let's push boundaries! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Research-Grade Self-Reflecting Agent Setup\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "class ResearchAgent:\n",
    "    def __init__(self):\n",
    "        self.trace_history = []\n",
    "        self.meta_insights = []\n",
    "    \n",
    "    def generate_with_confidence(self, problem, strategy='balanced'):\n",
    "        \"\"\"Generate response with confidence scoring\"\"\"\n",
    "        responses = {\n",
    "            'conservative': f\"Careful analysis of '{problem}': Start with proven approaches, validate each step, minimize risks.\",\n",
    "            'aggressive': f\"Bold approach to '{problem}': Push boundaries, try novel solutions, accept calculated risks.\",\n",
    "            'balanced': f\"Balanced solution for '{problem}': Combine proven methods with innovative elements, validate thoroughly.\"\n",
    "        }\n",
    "        \n",
    "        response = responses.get(strategy, responses['balanced'])\n",
    "        confidence = random.uniform(0.7, 0.95)  # Simulate confidence scoring\n",
    "        \n",
    "        return response, confidence\n",
    "    \n",
    "    def multi_perspective_evaluate(self, response):\n",
    "        \"\"\"Evaluate from multiple perspectives\"\"\"\n",
    "        perspectives = {\n",
    "            'technical': self._evaluate_technical(response),\n",
    "            'practical': self._evaluate_practical(response), \n",
    "            'innovative': self._evaluate_innovative(response)\n",
    "        }\n",
    "        return perspectives\n",
    "    \n",
    "    def _evaluate_technical(self, response):\n",
    "        score = 0.8 if any(word in response.lower() for word in ['algorithm', 'complexity', 'data', 'system']) else 0.6\n",
    "        return {'score': score, 'feedback': 'Technical depth assessment'}\n",
    "    \n",
    "    def _evaluate_practical(self, response):\n",
    "        score = 0.9 if any(word in response.lower() for word in ['step', 'implement', 'validate', 'test']) else 0.5\n",
    "        return {'score': score, 'feedback': 'Practical applicability check'}\n",
    "    \n",
    "    def _evaluate_innovative(self, response):\n",
    "        score = 0.7 if any(word in response.lower() for word in ['novel', 'innovative', 'creative', 'bold']) else 0.8\n",
    "        return {'score': score, 'feedback': 'Innovation level analysis'}\n",
    "    \n",
    "    def meta_reflect(self, problem, solution_trace):\n",
    "        \"\"\"Reflect on the reflection process itself\"\"\"\n",
    "        analysis = f\"\"\"\n",
    "        Meta-Analysis for \"{problem}\":\n",
    "        \n",
    "        Process Quality: {'Excellent' if len(solution_trace) <= 3 else 'Could be more efficient'}\n",
    "        Convergence: {'Fast' if len(solution_trace) <= 2 else 'Gradual'}\n",
    "        Strategy Effectiveness: {random.choice(['High', 'Medium', 'Variable'])}\n",
    "        \n",
    "        Recommendations:\n",
    "        - {'Continue current approach' if len(solution_trace) <= 2 else 'Consider strategy adjustment'}\n",
    "        - Focus on {random.choice(['technical depth', 'practical steps', 'innovative thinking'])}\n",
    "        - {random.choice(['Increase', 'Maintain', 'Optimize'])} iteration count\n",
    "        \"\"\"\n",
    "        \n",
    "        self.meta_insights.append(analysis)\n",
    "        return analysis\n",
    "    \n",
    "    def research_solve(self, problem, max_iterations=3):\n",
    "        \"\"\"Research-grade solving with advanced features\"\"\"\n",
    "        print(f\"🔬 Research Problem: {problem}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        trace = []\n",
    "        \n",
    "        for i in range(max_iterations):\n",
    "            print(f\"\\n🔄 Research Iteration {i+1}\")\n",
    "            \n",
    "            # Generate with confidence\n",
    "            response, confidence = self.generate_with_confidence(problem)\n",
    "            print(f\"💡 Response (confidence: {confidence:.2f}): {response[:80]}...\")\n",
    "            \n",
    "            # Multi-perspective evaluation\n",
    "            evaluations = self.multi_perspective_evaluate(response)\n",
    "            avg_score = np.mean([eval_data['score'] for eval_data in evaluations.values()])\n",
    "            \n",
    "            print(f\"🔍 Multi-Perspective Score: {avg_score:.2f}\")\n",
    "            for perspective, eval_data in evaluations.items():\n",
    "                print(f\"  • {perspective}: {eval_data['score']:.2f}\")\n",
    "            \n",
    "            trace.append({\n",
    "                'iteration': i+1,\n",
    "                'response': response,\n",
    "                'confidence': confidence,\n",
    "                'avg_score': avg_score,\n",
    "                'evaluations': evaluations\n",
    "            })\n",
    "            \n",
    "            # Early stopping if high quality\n",
    "            if avg_score > 0.85 and confidence > 0.9:\n",
    "                print(\"✅ High quality achieved - stopping early\")\n",
    "                break\n",
    "                \n",
    "            # Adaptive refinement\n",
    "            if avg_score < 0.7:\n",
    "                response = f\"[ENHANCED] {response} [Added depth and validation based on multi-perspective analysis]\"\n",
    "        \n",
    "        # Meta-reflection\n",
    "        meta_analysis = self.meta_reflect(problem, trace)\n",
    "        print(f\"\\n🧠 Meta-Reflection:\")\n",
    "        print(meta_analysis)\n",
    "        \n",
    "        self.trace_history.append(trace)\n",
    "        return trace[-1]['response'] if trace else \"No solution generated\"\n",
    "\n",
    "# Initialize research agent\n",
    "research_agent = ResearchAgent()\n",
    "print(\"🔬 Research Agent initialized!\")\n",
    "print(\"🧠 Advanced features: confidence scoring, multi-perspective evaluation, meta-reflection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🧠 Experiment 1: Meta-Reflection\n",
    "\n",
    "**Research Question:** Can AI improve by analyzing its own thinking process?\n",
    "\n",
    "This experiment runs two problems through both a meta-reflective agent and a research agent, then displays the final result and a collapsible trace for each. Meta-reflection analysis is also shown in a collapsible block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-Reflection Implementation (Streamlined)\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "class MetaReflectiveAgent:\n",
    "    \"\"\"Agent that can reflect on its own reflection process\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.meta_trace = []  # Track reflection strategy evolution\n",
    "        self.strategies = ['technical', 'creative', 'systematic']\n",
    "        self.evaluations = ['critical', 'constructive', 'comprehensive']\n",
    "    \n",
    "    def simple_solve(self, problem: str, max_iterations: int = 2) -> List[Dict]:\n",
    "        \"\"\"Simple solve method that returns a trace for meta-analysis\"\"\"\n",
    "        trace = []\n",
    "        \n",
    "        # Simulate a solving process\n",
    "        response = f\"Initial approach to '{problem}': Start with basic solution.\"\n",
    "        for i in range(max_iterations):\n",
    "            critique = f\"Iteration {i+1} critique: Could be more detailed and specific.\"\n",
    "            refined = f\"Refined iteration {i+1}: {response} Added more technical depth and examples.\"\n",
    "            \n",
    "            trace.append({\n",
    "                'iteration': i+1,\n",
    "                'response': response,\n",
    "                'critique': critique,\n",
    "                'refined': refined\n",
    "            })\n",
    "            response = refined\n",
    "        \n",
    "        return trace\n",
    "    \n",
    "    def meta_reflect(self, problem: str, reflection_trace: List[Dict]) -> str:\n",
    "        \"\"\"Analyze the quality of the reflection process itself\"\"\"\n",
    "        \n",
    "        # Simulate meta-reflection analysis\n",
    "        meta_analysis = self._simulate_meta_reflection(problem, reflection_trace)\n",
    "        \n",
    "        self.meta_trace.append({\n",
    "            'problem': problem,\n",
    "            'reflection_trace': reflection_trace,\n",
    "            'meta_analysis': meta_analysis\n",
    "        })\n",
    "        \n",
    "        return meta_analysis\n",
    "    \n",
    "    def _simulate_meta_reflection(self, problem: str, trace: List[Dict]) -> str:\n",
    "        \"\"\"Simulate meta-reflection analysis\"\"\"\n",
    "        effectiveness = 'good' if len(trace) <= 3 else 'excessive'\n",
    "        strategy_fit = 'appropriate' if 'technical' in problem.lower() else 'could be improved'\n",
    "        progression = 'improved' if len(trace) > 1 else 'unclear'\n",
    "        focus = 'clear' if len(trace) >= 2 else 'limited'\n",
    "        confidence = np.random.randint(70, 95)\n",
    "        \n",
    "        return f\"\"\"\n",
    "        **Meta-Reflection Analysis:**\n",
    "        \n",
    "        **Effectiveness**: The reflection process showed {effectiveness} convergence\n",
    "        for this problem type. The critique strategy was {strategy_fit}\n",
    "        for the domain.\n",
    "        \n",
    "        **Quality Progression**: Response quality {progression} \n",
    "        across iterations with {focus} refinement focus.\n",
    "        \n",
    "        **Recommendations**: \n",
    "        1. Consider domain-specific evaluation criteria\n",
    "        2. Implement early stopping for convergence\n",
    "        3. Add confidence scoring to responses\n",
    "        4. Explore alternative refinement strategies\n",
    "        \n",
    "        **Confidence**: {confidence}% confident in this meta-analysis\n",
    "        \"\"\"\n",
    "\n",
    "# Initialize meta-reflective agent\n",
    "meta_agent = MetaReflectiveAgent()\n",
    "print(\"🔄 Meta-Reflective Agent initialized!\")\n",
    "print(\"🧠 Ready for second-order reflection experiments!\")\n",
    "\n",
    "# --- Problem 1 ---\n",
    "research_problem_1 = \"How do I design a fault-tolerant distributed system?\"\n",
    "trace_1 = meta_agent.simple_solve(research_problem_1, max_iterations=3)\n",
    "result_1 = research_agent.research_solve(research_problem_1, max_iterations=3)\n",
    "\n",
    "print(\"\\n🎯 Final Result (Problem 1):\")\n",
    "print(result_1)\n",
    "\n",
    "trace_md = \"\"\"<details><summary>🔎 Show Full Reflection Trace (Problem 1)</summary>\\n\"\"\"\n",
    "for step in trace_1:\n",
    "    trace_md += f\"\\n<b>Iteration {step['iteration']}</b><br>\"\n",
    "    trace_md += f\"<b>Response:</b> {step['response']}<br>\"\n",
    "    if 'critique' in step:\n",
    "        trace_md += f\"<b>Critique:</b> {step['critique']}<br>\"\n",
    "    if 'refined' in step:\n",
    "        trace_md += f\"<b>Refined:</b> {step['refined']}<br>\"\n",
    "    trace_md += \"<hr>\"\n",
    "trace_md += \"</details>\"\n",
    "display(Markdown(trace_md))\n",
    "\n",
    "# --- Problem 2 ---\n",
    "research_problem_2 = \"What's the best approach for real-time machine learning?\"\n",
    "trace_2 = meta_agent.simple_solve(research_problem_2, max_iterations=2)\n",
    "result_2 = research_agent.research_solve(research_problem_2, max_iterations=2)\n",
    "\n",
    "print(\"\\n🎯 Final Result (Problem 2):\")\n",
    "print(result_2)\n",
    "\n",
    "trace_md2 = \"\"\"<details><summary>🔎 Show Full Reflection Trace (Problem 2)</summary>\\n\"\"\"\n",
    "for step in trace_2:\n",
    "    trace_md2 += f\"\\n<b>Iteration {step['iteration']}</b><br>\"\n",
    "    trace_md2 += f\"<b>Response:</b> {step['response']}<br>\"\n",
    "    if 'critique' in step:\n",
    "        trace_md2 += f\"<b>Critique:</b> {step['critique']}<br>\"\n",
    "    if 'refined' in step:\n",
    "        trace_md2 += f\"<b>Refined:</b> {step['refined']}<br>\"\n",
    "    trace_md2 += \"<hr>\"\n",
    "trace_md2 += \"</details>\"\n",
    "display(Markdown(trace_md2))\n",
    "\n",
    "# --- Meta-Reflection Analysis ---\n",
    "meta_analysis_1 = meta_agent.meta_reflect(research_problem_1, trace_1)\n",
    "meta_analysis_2 = meta_agent.meta_reflect(research_problem_2, trace_2)\n",
    "\n",
    "meta_md = \"\"\"<details><summary>📊 Show Meta-Reflection Analysis</summary>\\n\"\"\"\n",
    "meta_md += f\"<pre>{meta_analysis_1}</pre>\"\n",
    "meta_md += \"<hr>\"\n",
    "meta_md += f\"<pre>{meta_analysis_2}</pre>\"\n",
    "meta_md += \"</details>\"\n",
    "display(Markdown(meta_md))\n",
    "\n",
    "print(\"\\n📊 Meta-Reflection Analysis:\")\n",
    "print(f\"Total experiments conducted: {len(research_agent.trace_history)}\")\n",
    "print(f\"Meta-insights generated: {len(research_agent.meta_insights)}\")\n",
    "print(\"\\n✨ Key Finding: Meta-reflection helps identify when to stop iterating!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 📈 Experiment 2: Performance Analysis\n",
    "\n",
    "**Research Question:** How do different strategies perform across problem types?\n",
    "\n",
    "This experiment compares strategies on several problems, showing concise metrics and a performance plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Analysis (Streamlined)\n",
    "class PerformanceAnalyzer:\n",
    "    \"\"\"Analyze and compare the performance of different strategies in problem-solving\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "    \n",
    "    def analyze_strategy_performance(self, problems, strategies):\n",
    "        \"\"\"Compare strategy performance across problems\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for problem in problems:\n",
    "            results[problem] = {}\n",
    "            for strategy in strategies:\n",
    "                response, confidence = research_agent.generate_with_confidence(problem, strategy)\n",
    "                evaluations = research_agent.multi_perspective_evaluate(response)\n",
    "                avg_score = np.mean([eval_data['score'] for eval_data in evaluations.values()])\n",
    "                \n",
    "                results[problem][strategy] = {\n",
    "                    'confidence': confidence,\n",
    "                    'quality_score': avg_score,\n",
    "                    'response_length': len(response)\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def plot_performance(self, results):\n",
    "        \"\"\"Visualize performance comparison\"\"\"\n",
    "        strategies = list(next(iter(results.values())).keys())\n",
    "        problems = list(results.keys())\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Quality scores\n",
    "        quality_data = []\n",
    "        for strategy in strategies:\n",
    "            scores = [results[problem][strategy]['quality_score'] for problem in problems]\n",
    "            quality_data.append(scores)\n",
    "            ax1.plot(range(len(problems)), scores, marker='o', label=strategy)\n",
    "        \n",
    "        ax1.set_title('Strategy Quality Comparison')\n",
    "        ax1.set_xlabel('Problem Index')\n",
    "        ax1.set_ylabel('Quality Score')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Confidence levels\n",
    "        for strategy in strategies:\n",
    "            confidence = [results[problem][strategy]['confidence'] for problem in problems]\n",
    "            ax2.plot(range(len(problems)), confidence, marker='s', label=strategy)\n",
    "        \n",
    "        ax2.set_title('Strategy Confidence Comparison')\n",
    "        ax2.set_xlabel('Problem Index')\n",
    "        ax2.set_ylabel('Confidence Level')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return quality_data\n",
    "\n",
    "# Run performance analysis\n",
    "analyzer = PerformanceAnalyzer()\n",
    "\n",
    "test_problems = [\n",
    "    \"Optimize database query performance\",\n",
    "    \"Implement secure authentication system\", \n",
    "    \"Design scalable microservices architecture\",\n",
    "    \"Build real-time data processing pipeline\"\n",
    "]\n",
    "\n",
    "test_strategies = ['conservative', 'aggressive', 'balanced']\n",
    "\n",
    "print(\"📈 Running Performance Analysis...\")\n",
    "performance_results = analyzer.analyze_strategy_performance(test_problems, test_strategies)\n",
    "\n",
    "print(\"\\n📉 Performance Summary:\")\n",
    "for i, problem in enumerate(test_problems):\n",
    "    print(f\"\\nProblem {i+1}: {problem[:40]}...\")\n",
    "    for strategy in test_strategies:\n",
    "        data = performance_results[problem][strategy]\n",
    "        print(f\"  {strategy}: Quality={data['quality_score']:.2f}, Confidence={data['confidence']:.2f}\")\n",
    "\n",
    "quality_data = analyzer.plot_performance(performance_results)\n",
    "\n",
    "print(\"\\n✨ Analysis Insights:\")\n",
    "print(\"- Different strategies excel at different problem types\")\n",
    "print(\"- Balanced approach shows consistent performance\")\n",
    "print(\"- Conservative strategy has high confidence but may lack innovation\")\n",
    "print(\"- Aggressive strategy shows variable results but high potential\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ⚖️ Experiment 3: Research Applications\n",
    "\n",
    "**Research Question:** What are the real-world applications of self-reflecting AI?\n",
    "\n",
    "This section demonstrates four practical applications with concise, clear output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research Applications (Streamlined)\n",
    "class ApplicationDemo:\n",
    "    def __init__(self):\n",
    "        self.applications = {\n",
    "            'code_review': self._code_review_agent,\n",
    "            'research_assistant': self._research_assistant,\n",
    "            'system_design': self._system_design_helper,\n",
    "            'debugging_helper': self._debugging_assistant\n",
    "        }\n",
    "    \n",
    "    def _code_review_agent(self, code_snippet):\n",
    "        \"\"\"AI code reviewer with self-reflection\"\"\"\n",
    "        review = f\"Code Review for: {code_snippet[:50]}...\\n\"\n",
    "        review += \"Issues found: Style inconsistencies, missing error handling\\n\"\n",
    "        review += \"Suggestions: Add comments, implement try-catch blocks\\n\"\n",
    "        review += \"Reflection: This review covers basic issues but should also check performance and security\"\n",
    "        return review\n",
    "    \n",
    "    def _research_assistant(self, research_topic):\n",
    "        \"\"\"AI research assistant with critical thinking\"\"\"\n",
    "        analysis = f\"Research Analysis: {research_topic}\\n\"\n",
    "        analysis += \"Key areas to explore: Literature review, methodology, current gaps\\n\"\n",
    "        analysis += \"Critical questions: What assumptions are being made? What evidence supports this?\\n\"\n",
    "        analysis += \"Self-critique: Need to consider bias in sources and alternative perspectives\"\n",
    "        return analysis\n",
    "    \n",
    "    def _system_design_helper(self, system_requirements):\n",
    "        \"\"\"AI system architect with validation\"\"\"\n",
    "        design = f\"System Design for: {system_requirements}\\n\"\n",
    "        design += \"Architecture: Microservices with API gateway, database clustering\\n\"\n",
    "        design += \"Validation: Load testing, security audit, scalability analysis\\n\"\n",
    "        design += \"Reflection: Design looks solid but should consider disaster recovery scenarios\"\n",
    "        return design\n",
    "    \n",
    "    def _debugging_assistant(self, bug_description):\n",
    "        \"\"\"AI debugging helper with systematic approach\"\"\"\n",
    "        debug_plan = f\"Debug Strategy for: {bug_description}\\n\"\n",
    "        debug_plan += \"Steps: 1) Reproduce issue, 2) Check logs, 3) Test hypotheses\\n\"\n",
    "        debug_plan += \"Tools: Debugger, profiler, logging framework\\n\"\n",
    "        debug_plan += \"Self-check: Am I testing the right scenarios? Are there edge cases missed?\"\n",
    "        return debug_plan\n",
    "    \n",
    "    def demonstrate_application(self, app_type, input_data):\n",
    "        \"\"\"Demonstrate a specific application\"\"\"\n",
    "        if app_type in self.applications:\n",
    "            result = self.applications[app_type](input_data)\n",
    "            return result\n",
    "        return \"Application not available\"\n",
    "\n",
    "# Demo the applications\n",
    "demo = ApplicationDemo()\n",
    "\n",
    "print(\"🛠️ Real-World Applications Demo:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Code Review Demo\n",
    "print(\"\\n1. 📝 Code Review Assistant:\")\n",
    "code_sample = \"def calculate_fibonacci(n): return n if n <= 1 else calculate_fibonacci(n-1) + calculate_fibonacci(n-2)\"\n",
    "code_review_result = demo.demonstrate_application('code_review', code_sample)\n",
    "print(code_review_result)\n",
    "\n",
    "# Research Assistant Demo\n",
    "print(\"\\n2. 🔬 Research Assistant:\")\n",
    "research_topic = \"The effectiveness of self-reflecting AI agents in educational software\"\n",
    "research_result = demo.demonstrate_application('research_assistant', research_topic)\n",
    "print(research_result)\n",
    "\n",
    "# System Design Demo\n",
    "print(\"\\n3. 🏢 System Design Helper:\")\n",
    "system_req = \"E-commerce platform handling 1M+ users with real-time inventory\"\n",
    "design_result = demo.demonstrate_application('system_design', system_req)\n",
    "print(design_result)\n",
    "\n",
    "# Debugging Demo\n",
    "print(\"\\n4. 🐛 Debugging Assistant:\")\n",
    "bug_desc = \"API response time increased 300% after latest deployment\"\n",
    "debug_result = demo.demonstrate_application('debugging_helper', bug_desc)\n",
    "print(debug_result)\n",
    "\n",
    "print(\"\\n✨ Applications Summary:\")\n",
    "print(\"• Code Review: Catches issues humans might miss\")\n",
    "print(\"• Research: Provides systematic analysis framework\")\n",
    "print(\"• System Design: Validates architecture decisions\")\n",
    "print(\"• Debugging: Offers structured problem-solving approach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 🎯 Your Turn: Custom Research Experiment\n",
    "\n",
    "Try your own research experiment! Edit the variables below, run the cell, and inspect the results and trace. The trace is shown in a collapsible block for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Research Experiment (Streamlined)\n",
    "# Edit these variables to try your own experiment!\n",
    "custom_problem = \"How can AI help with climate change?\"  # Change this!\n",
    "custom_strategy = 'balanced'  # Try: 'conservative', 'aggressive', 'balanced'\n",
    "custom_iterations = 3  # Adjust 1-5\n",
    "\n",
    "print(\"🎯 Your Custom Research Experiment:\")\n",
    "print(f\"Problem: {custom_problem}\")\n",
    "print(f\"Strategy: {custom_strategy}\")\n",
    "\n",
    "trace = []\n",
    "def capture_trace(problem, strategy, max_iterations):\n",
    "    responses = []\n",
    "    for i in range(max_iterations):\n",
    "        response, confidence = research_agent.generate_with_confidence(problem, strategy)\n",
    "        evaluations = research_agent.multi_perspective_evaluate(response)\n",
    "        avg_score = np.mean([eval_data['score'] for eval_data in evaluations.values()])\n",
    "        responses.append({\n",
    "            'iteration': i+1,\n",
    "            'response': response,\n",
    "            'confidence': confidence,\n",
    "            'avg_score': avg_score,\n",
    "            'evaluations': evaluations\n",
    "        })\n",
    "        if avg_score > 0.85 and confidence > 0.9:\n",
    "            break\n",
    "    return responses, responses[-1]['response'] if responses else \"No solution generated\"\n",
    "\n",
    "trace, custom_result = capture_trace(custom_problem, custom_strategy, custom_iterations)\n",
    "\n",
    "print(\"\\n🎯 Final Result:\")\n",
    "print(custom_result)\n",
    "\n",
    "print(f\"\\n📊 Custom Experiment Analysis:\")\n",
    "print(f\"• Iterations: {len(trace)}\")\n",
    "print(f\"• Final confidence: {trace[-1]['confidence']:.2f}\")\n",
    "print(f\"• Final avg score: {trace[-1]['avg_score']:.2f}\")\n",
    "\n",
    "trace_md = \"\"\"<details><summary>🔎 Show Full Reflection Trace</summary>\\n\"\"\"\n",
    "for step in trace:\n",
    "    trace_md += f\"\\n<b>Iteration {step['iteration']}</b><br>\"\n",
    "    trace_md += f\"<b>Response:</b> {step['response']}<br>\"\n",
    "    trace_md += f\"<b>Confidence:</b> {step['confidence']:.2f}<br>\"\n",
    "    trace_md += f\"<b>Avg Score:</b> {step['avg_score']:.2f}<br>\"\n",
    "    trace_md += \"<b>Evaluations:</b><ul>\"\n",
    "    for k, v in step['evaluations'].items():\n",
    "        trace_md += f\"<li>{k}: {v['score']:.2f} ({v['feedback']})</li>\"\n",
    "    trace_md += \"</ul><hr>\"\n",
    "trace_md += \"</details>\"\n",
    "display(Markdown(trace_md))\n",
    "\n",
    "print(\"\\n✨ Try These Modifications:\")\n",
    "print(\"1. Change the custom_problem to something you're curious about\")\n",
    "print(\"2. Try different strategy combinations\")\n",
    "print(\"3. Adjust custom_iterations (1-5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 🎆 Research Conclusions & Your Next Steps\n",
    "\n",
    "### What We've Discovered\n",
    "- **Meta-reflection works**: AI can improve by analyzing its own processes\n",
    "- **Strategy matters**: Different approaches work for different problems\n",
    "- **Multi-perspective evaluation**: Catches more issues than single-criteria review\n",
    "- **Real applications exist**: Code review, research, design, debugging\n",
    "\n",
    "### Your Research Journey\n",
    "- **Join AI research projects**: Work on real problems with faculty\n",
    "- **Read cutting-edge papers**: ArXiv, NeurIPS, ICML conferences\n",
    "- **Build your own applications**: Implement these ideas in real projects\n",
    "- **Contribute to open source**: LangChain, AutoGPT, research frameworks\n",
    "\n",
    "### Recommended Next Reading\n",
    "- \"Constitutional AI\" (Anthropic) - AI with embedded principles\n",
    "- \"Self-Refine\" (Madaan et al.) - Iterative improvement techniques\n",
    "- \"Reflexion\" (Shinn et al.) - Learning from failures\n",
    "- \"Chain-of-Thought\" (Wei et al.) - Reasoning in language models\n",
    "\n",
    "### Research Project Ideas\n",
    "- **Domain-specific agents**: Build agents for specific CS fields\n",
    "- **Evaluation metrics**: Develop better ways to measure reflection quality\n",
    "- **Human-AI collaboration**: Combine human insight with AI reflection\n",
    "- **Efficiency optimization**: Reduce computational cost of reflection\n",
    "\n",
    "### 🌟 Join the UofT AI Agents Club!\n",
    "This workshop is just the beginning. The club works on:\n",
    "- Research projects with industry partners\n",
    "- AI competitions and hackathons\n",
    "- Study groups for cutting-edge papers\n",
    "- Startup opportunities in AI agents space\n",
    "\n",
    "**Ready to push the boundaries of AI? The future needs researchers like you!** 🧠✨\n",
    "\n",
    "---\n",
    "\n",
    "*\"The best way to predict the future is to invent it.\" - Alan Kay*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
